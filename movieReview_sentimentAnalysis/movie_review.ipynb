{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8cbd6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aditi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aditi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ccf10e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"IMDB_Dataset.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fc9c694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ae6efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_text(text):\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # Remove HTML tags\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^0-9a-zA-Z\\s]', ' ', text)  # Remove non-alphanumeric characters\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88e54cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review_clean'] = train['review'].apply(processing_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42188912",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Handling Negations\n",
    "Sentiment analysis heavily relies on words like \"not,\" \"no,\" and \"never.\" \n",
    "The default NLTK stopwords list includes these! Removing \"not\" can turn a sentence like \"not good\" into \"good,\" completely flipping its meaning.\n",
    "Create a custom stop words list that excludes negation words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95b48301",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words_to_keep = {\"not\", \"no\", \"nor\", \"ain\", \"aren\", \"couldn\", \"didn\", \"doesn\", \"hadn\", \"hasn\", \"haven\", \"isn\", \n",
    "\"mightn\", \"mustn\", \"needn\", \"shan\", \"shouldn\", \"wasn\", \"weren\", \"won\", \"wouldn\"}\n",
    "custom_stop_words = stop_words - words_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),      # Use both unigrams and bigrams\n",
    "    max_features=15000,      # Keep only the top 15,000 most frequent features\n",
    "    min_df = 5,              # Ignore terms that appear in less than 5 documents\n",
    "    stop_words=list(custom_stop_words)\n",
    ")\n",
    "## vectorize the clean text\n",
    "X = vectorizer.fit_transform(train[\"review_clean\"])\n",
    "y = train['sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e64747a8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## stratify:  ## stratify meaning proportion of different classes in traget variable (y) is the same in both your training and testing sets\n",
    "\n",
    "## C: C is the regularization strength for Logistic Regression, higher values mean less regularization and overfitting,\n",
    " model is memorizing the data instead of learning from data\n",
    "\n",
    "## Solver: specific optimization algorithm used to find the best-fitting model parameters (weights or coefficients).\n",
    " Its job is to minimize the cost function\n",
    "1. Logistic Regression: A highly optimized library for linear classification, use when small dataset, support L1 and L2 regularization\n",
    "2. lbfgs: It approximates the second derivative of the cost function to make better steps toward the minimum, it is a default\n",
    "            particularly useful for multiclass classification problem, support only L2 regularization\n",
    "3. newton-cg: A Newton-Conjugate Gradient algorithm. Like 'lbfgs', it uses information about the second derivative, making it computationally intensive.\n",
    "    multiclass problem and the number of features isn't vastly larger than the number of samples\n",
    "5. sag: Stochastic Average Gradient descent, use when large dataset, only L2 regularization, require features on similar scales\n",
    "6. saga: A variant of SAG that supports both L1 and L2 regularization, use when large dataset, use when very large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b342399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Naive Bayes --\n",
      "Accuracy:, 86.58%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.85      0.88      0.87      4961\n",
      "    Positive       0.88      0.85      0.86      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "-- Logistic Regression --\n",
      "Accuracy:, 89.53%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.88      0.89      4961\n",
      "    Positive       0.88      0.91      0.90      5039\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Naive Bayes model\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "y_pred_nb = nb_clf.predict(X_test)\n",
    "print(\"\\n-- Naive Bayes --\")\n",
    "print(f\"Accuracy:, {accuracy_score(y_test, y_pred_nb)* 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred_nb, target_names=['Negative', 'Positive']))\n",
    "\n",
    "## Logistic Regression\n",
    "lr_clf = LogisticRegression(solver='liblinear', C=1.0, random_state=42)   ## C is the regularization strength, higher number meaning less regularization and overfitting\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred_lr = lr_clf.predict(X_test)\n",
    "print(\"\\n-- Logistic Regression --\")\n",
    "print(f\"Accuracy:, {accuracy_score(y_test, y_pred_lr)* 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['Negative', 'Positive']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
