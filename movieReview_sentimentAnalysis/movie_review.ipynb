{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cbd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf10e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"IMDB_Dataset.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_text(text):\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # Remove HTML tags\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^0-9a-zA-Z\\s]', ' ', text)  # Remove non-alphanumeric characters\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e54cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review_clean'] = train['review'].apply(processing_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42188912",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Handling Negations\n",
    "Sentiment analysis heavily relies on words like \"not,\" \"no,\" and \"never.\" \n",
    "The default NLTK stopwords list includes these! Removing \"not\" can turn a sentence like \"not good\" into \"good,\" completely flipping its meaning.\n",
    "Create a custom stop words list that excludes negation words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b48301",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words_to_keep = {\"not\", \"no\", \"nor\", \"ain\", \"aren\", \"couldn\", \"didn\", \"doesn\", \"hadn\", \"hasn\", \"haven\", \"isn\", \n",
    "\"mightn\", \"mustn\", \"needn\", \"shan\", \"shouldn\", \"wasn\", \"weren\", \"won\", \"wouldn\"}\n",
    "custom_stop_words = stop_words - words_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),      # Use both unigrams and bigrams\n",
    "    max_features=15000,      # Keep only the top 15,000 most frequent features\n",
    "    min_df = 5,              # Ignore terms that appear in less than 5 documents\n",
    "    stop_words=list(custom_stop_words)\n",
    ")\n",
    "## vectorize the clean text\n",
    "X = vectorizer.fit_transform(train[\"review_clean\"])\n",
    "y = train['sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e64747a8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## stratify:  ## stratify meaning proportion of different classes in traget variable (y) is the same in both your training and testing sets\n",
    "\n",
    "## C: C is the regularization strength for Logistic Regression, higher values mean less regularization and overfitting,\n",
    " model is memorizing the data instead of learning from data\n",
    "\n",
    "## Solver: specific optimization algorithm used to find the best-fitting model parameters (weights or coefficients).\n",
    " Its job is to minimize the cost function\n",
    "1. Logistic Regression: A highly optimized library for linear classification, use when small dataset, support L1 and L2 regularization\n",
    "2. lbfgs: It approximates the second derivative of the cost function to make better steps toward the minimum, it is a default\n",
    "            particularly useful for multiclass classification problem, support only L2 regularization\n",
    "3. newton-cg: A Newton-Conjugate Gradient algorithm. Like 'lbfgs', it uses information about the second derivative, making it computationally intensive.\n",
    "    multiclass problem and the number of features isn't vastly larger than the number of samples\n",
    "5. sag: Stochastic Average Gradient descent, use when large dataset, only L2 regularization, require features on similar scales\n",
    "6. saga: A variant of SAG that supports both L1 and L2 regularization, use when large dataset, use when very large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3f2eb0",
   "metadata": {},
   "source": [
    "## Model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b342399",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naive Bayes model\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "y_pred_nb = nb_clf.predict(X_test)\n",
    "print(\"\\n-- Naive Bayes --\")\n",
    "print(f\"Accuracy:, {accuracy_score(y_test, y_pred_nb)* 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred_nb, target_names=['Negative', 'Positive']))\n",
    "\n",
    "## Logistic Regression\n",
    "lr_clf = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred_lr = lr_clf.predict(X_test)\n",
    "print(\"\\n-- Logistic Regression --\")\n",
    "print(f\"Accuracy:, {accuracy_score(y_test, y_pred_lr)* 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb415596",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7284402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'penalty': ['l1', 'l2'],               ## liblinear support both l1 and l2\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],                     ## lbfgs supports only l2\n",
    "        'solver': ['lbfgs'],\n",
    "        'C': [0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09abde5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "\n",
      "Best parameters found:  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "89.42% accuracy on training set\n",
      "89.64% accuracy on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.89      0.89      4961\n",
      "    Positive       0.89      0.91      0.90      5039\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator=lr_model,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,           ## number of cross-validation folds\n",
    "                           verbose=1,     \n",
    "                           n_jobs=-1)      ## use all available cpus\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"\\nBest parameters found: \", grid_search.best_params_)\n",
    "print(f\"{grid_search.best_score_ * 100:.2f}% accuracy on training set\")\n",
    "\n",
    "## test set evaluation\n",
    "best_lr_clf = grid_search.best_estimator_\n",
    "y_pred_best_lr = best_lr_clf.predict(X_test)\n",
    "print(f\"{accuracy_score(y_test, y_pred_best_lr) * 100:.2f}% accuracy on test set\")\n",
    "print(classification_report(y_test, y_pred_best_lr, target_names=['Negative', 'Positive']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
